{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "method \u003d \"google\"\n",
        "max_episodes \u003d \u0027none\u0027\n",
        "\n",
        "import os\n",
        "\n",
        "import dataiku\n",
        "import pandas as pd, numpy as np\n",
        "from dataiku import pandasutils as pdu\n",
        "from dataiku.customrecipe import *\n",
        "from deepspeech import Model\n",
        "import scipy.io.wavfile as wav\n",
        "import speech_recognition as sr\n",
        "\n",
        "\n",
        "## DEEPSPEECH PARAMETERS\n",
        "BEAM_WIDTH\u003d3000\n",
        "LM_WEIGHT \u003d0.50\n",
        "WORD_COUNT_WEIGHT\u003d1\n",
        "VALID_WORD_COUNT_WEIGHT\u003d5\n",
        "\n",
        "# These constants are tied to the shape of the graph used (changing them changes\n",
        "# the geometry of the first layer), so make sure you use the same constants that\n",
        "# were used during training\n",
        "# Number of MFCC features to use\n",
        "N_FEATURES \u003d 26\n",
        "# Size of the context window used for producing timesteps in the input vector\n",
        "N_CONTEXT \u003d 9\n",
        "\n",
        "# Read recipe inputs\n",
        "audios \u003d dataiku.Folder(\"episode_wavs_hdfs\")\n",
        "weights \u003d dataiku.Folder(\"deepspeech_model\").get_path()\n",
        "\n",
        "model \u003d os.path.join(weights, \u0027deepspeech-0.6.1-models\u0027, \u0027output_graph.pb\u0027)\n",
        "alphabet \u003d os.path.join(weights, \u0027deepspeech-0.6.1-models\u0027, \u0027alphabet.txt\u0027)\n",
        "lm \u003d os.path.join(weights, \u0027deepspeech-0.6.1-models\u0027, \u0027lm.binary\u0027)\n",
        "trie \u003d os.path.join(weights, \u0027deepspeech-0.6.1-models\u0027, \u0027trie\u0027)\n",
        "\n",
        "#ds \u003d Model(model, N_FEATURES, N_CONTEXT, alphabet, int(BEAM_WIDTH))\n",
        "ds \u003d Model(model, int(BEAM_WIDTH))\n",
        "\n",
        "#ds.enableDecoderWithLM(alphabet, lm, trie, LM_WEIGHT,\n",
        "#                       WORD_COUNT_WEIGHT, VALID_WORD_COUNT_WEIGHT)\n",
        "ds.enableDecoderWithLM(lm, trie, LM_WEIGHT,WORD_COUNT_WEIGHT)\n",
        "\n",
        "\n",
        "df \u003d pd.DataFrame(columns\u003d[\u0027path\u0027, \u0027text\u0027])\n",
        "idx \u003d 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "r \u003d sr.Recognizer()\n",
        "with audios.get_download_stream(audios.list_paths_in_partition()[0][1:]) as stream:\n",
        "#    print(stream.__dict__)\n",
        "    with sr.AudioFile(stream) as source:\n",
        "        audio_google \u003d r.record(source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "text \u003d r.recognize_google(audio_google)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\u0027you listening to Drake SM\u0027"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "b.__class__"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "for path in audios.list_paths_in_partition():\n",
        "    path \u003d path[1:]\n",
        "    audio_path \u003d os.path.join(audios.get_path(), path)\n",
        "    print(audio_path)\n",
        "\n",
        "    if method \u003d\u003d \"deepspeech\":\n",
        "        fs, audio_ds \u003d wav.read(audio_path)\n",
        "        text \u003d ds.stt(audio_ds)\n",
        "    if method \u003d\u003d \"google\":\n",
        "        r \u003d sr.Recognizer()\n",
        "        with sr.AudioFile(audio_path) as source:\n",
        "            audio_google \u003d r.record(source)\n",
        "        try:\n",
        "            text \u003d r.recognize_google(audio_google)\n",
        "        except:\n",
        "            text \u003d \"NA\"\n",
        "    df.loc[idx] \u003d [path, text]\n",
        "    idx +\u003d 1\n",
        "\n",
        "    if max_episodes !\u003d \u0027none\u0027 \u0026 idx \u003d\u003d max_episodes:\n",
        "        break\n",
        "\n",
        "\n",
        "# Write recipe outputs\n",
        "detected_texts \u003d dataiku.Dataset(\"episodes_text\")\n",
        "detected_texts.write_with_schema(df)"
      ]
    }
  ],
  "metadata": {
    "associatedRecipe": "compute_episodes_text",
    "creator": "asamant",
    "customFields": {},
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": ""
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "tags": [
      "recipe-editor"
    ]
  },
  "nbformat": 4,
  "nbformat_minor": 1
}